{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Abhishek's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abhishek's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "#nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C:\\\\Users\\\\Abhishek's PC\\\\Desktop\\\\datasets\\\\HACKATHON-Text to key_words\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "root_path = os.getcwd()\n",
    "root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "glove_dir = 'glove'\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(data):\n",
    "    file_name = open(data, \"r\")\n",
    "    file = file_name.readlines()\n",
    "#     print(file)\n",
    "    for i in range(len(file)):\n",
    "        text= file[i].split(\". \")\n",
    "        sentences = []\n",
    "#         for i,sent in text:\n",
    "        for sentence in text:\n",
    "            sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "#             print(sentence)\n",
    "        sentences.pop() \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vectorizer(summ_data,embeddings_index):\n",
    "    summ_vec={}\n",
    "    count=1\n",
    "    for list_w in summ_data:\n",
    "        temp=[]\n",
    "        for word in list_w:\n",
    "            if word in embeddings_index.keys():\n",
    "                temp.append(embeddings_index[word])\n",
    "        summ_vec[count]=temp\n",
    "        count=count+1\n",
    "    return summ_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_binder_dict(vec):\n",
    "    summ_sent_vec={}\n",
    "    for num,list_w in vec.items():\n",
    "        n=1\n",
    "        temp=np.zeros(200)\n",
    "        for word in list_w:\n",
    "            temp=temp+word\n",
    "            n=n+1\n",
    "        summ_sent_vec[num]=(temp/n)\n",
    "    return summ_sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_binder(vec):\n",
    "    temp=np.zeros(200)\n",
    "    n=1\n",
    "    for num,list_w in vec.items():\n",
    "        temp=temp+list_w\n",
    "        n=n+1\n",
    "    doc_vec=(temp/n)\n",
    "        \n",
    "    return doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_bw_sent_doc_norm2(vec1,vec2):\n",
    "    dist_arr={}\n",
    "    for num,sent in vec1.items():\n",
    "        dist = np.linalg.norm(vec2-sent)\n",
    "        dist_arr[num]=dist\n",
    "    return dist_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_bw_sent_doc_cos(vec1,vec2):\n",
    "    dist_arr={}\n",
    "    for num,sent in vec1.items():\n",
    "        dist =cosine_distance(vec2,sent)\n",
    "        dist_arr[num]=dist\n",
    "    return dist_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer(doc_norm2_dist,joined_summ):\n",
    "    summarize_text = []\n",
    "    meaner=[]\n",
    "    for i in range(1,len(doc_norm2_dist)+1):\n",
    "        meaner.append(doc_norm2_dist[i])\n",
    "    mean_arr=np.array(meaner)\n",
    "    threshold=((mean_arr.mean()+np.median(mean_arr))/2)\n",
    "    for i in range(1,len(doc_norm2_dist)+1):\n",
    "        if doc_norm2_dist[i]<threshold:\n",
    "             summarize_text.append(\"\".join(joined_summ[i]))\n",
    "    return summarize_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Abhishek's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def corpus_maker(summarize_text):\n",
    "    corpus = []\n",
    "    for i in range(len(summarize_text)):\n",
    "        #Remove punctuations\n",
    "        text = re.sub('[^a-zA-Z]', ' ',summarize_text[i] )\n",
    "\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        #remove tags\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "\n",
    "        # remove special characters and digits\n",
    "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "\n",
    "        ##Convert to list from string\n",
    "        text = text.split()\n",
    "\n",
    "        ##Stemming\n",
    "        ps=PorterStemmer()\n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in  \n",
    "                stop_words] \n",
    "        text = \" \".join(text)\n",
    "        corpus.append(text)\n",
    "    return corpus \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_summ_key(path):\n",
    "    summ_data=read(path)\n",
    "    joined_summ={}\n",
    "    n=0\n",
    "    \n",
    "    for list_w in summ_data:\n",
    "        n=n+1\n",
    "        text=' '.join(list_w)\n",
    "        joined_summ[n]=text\n",
    "        \n",
    "    summ_vec=vectorizer(summ_data,embeddings_index)\n",
    "    summ_sent_vec=word_binder_dict(summ_vec)\n",
    "    doc_vec=doc_binder(summ_sent_vec)\n",
    "    doc_cos_dist=dist_bw_sent_doc_cos(summ_sent_vec,doc_vec)\n",
    "    doc_norm2_dist=dist_bw_sent_doc_norm2(summ_sent_vec,doc_vec)\n",
    "    \n",
    "    summarize_text=summarizer(doc_norm2_dist,joined_summ)\n",
    "    with open('summary.txt', 'w') as f:\n",
    "        for item in summarize_text:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "        \n",
    "    corpus=corpus_maker(summarize_text)\n",
    "    cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "    X=cv.fit_transform(corpus)\n",
    "    \n",
    "    n=5\n",
    "    \n",
    "    top_words = get_top_n_words(corpus, n)\n",
    "    top_df = pd.DataFrame(top_words)\n",
    "    top_df.columns=[\"Word\", \"Freq\"]\n",
    "    \n",
    "    keyword=[]\n",
    "    \n",
    "    for word in top_df['Word']:\n",
    "        keyword.append(word)\n",
    "    \n",
    "    top2_words = get_top_n2_words(corpus, n)\n",
    "    top2_df = pd.DataFrame(top2_words)\n",
    "    top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
    "    top2_df['Bi-gram']\n",
    "    \n",
    "    bi=[]\n",
    "    \n",
    "    for word in top2_df['Bi-gram']:\n",
    "        keyword.append(word)\n",
    "   \n",
    "    top3_words = get_top_n3_words(corpus, n)\n",
    "    top3_df = pd.DataFrame(top3_words)\n",
    "    top3_df.columns=[\"Tri-gram\", \"Freq\"]\n",
    "    \n",
    "    tri=[]\n",
    "    \n",
    "    for word in top3_df['Tri-gram']:\n",
    "        keyword.append(word)\n",
    "    with open('keyword.txt', 'w') as f:\n",
    "        for item in keyword:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "generate_summ_key(\"help_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
